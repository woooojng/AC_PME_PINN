{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0557ffda-0ae1-40d8-a916-ad933fbb8890",
   "metadata": {},
   "source": [
    "## Porous Medium Equation Solver Result Summary (1D)\n",
    "\n",
    "**Woojeong Kim** *7/30/2025*\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc70b61-8b8e-43af-8857-70821e080b3b",
   "metadata": {},
   "source": [
    "### Porous Medium Equation\n",
    "The **porous medium equation** is also called as **non-linear heat equation** and  has the form on 1-dimensional domain:\n",
    "$$\n",
    "\\frac{\\partial u(x,t)}{\\partial t} = \\Delta u(x,t)^m , \\text{~ where ~} m > 0 \\text{~and~} x \\in \\Omega.\n",
    "$$\n",
    "Main driving force of this equation is Energy term - i.e, the output solution variable $u$ behaves mainly by Energy of the form:\n",
    "$$\n",
    "E(u) = \\int_{\\Omega} u^m du\n",
    "$$\n",
    "To analyze the Energy, we take derivative as:\n",
    "$$\n",
    "\\frac{d E(u)}{dx} = m u^{m-1}u_x \n",
    "$$\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"../result_figures/exact_solution.png\" width=\"750\"/><br/>\n",
    "      <small>Figure 1.2: Energy adaptive</small>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede071d-bbae-4161-9834-534030c7924b",
   "metadata": {},
   "source": [
    "### Barenblatt-Kompaneets-Zeldovich similarity solution\n",
    "The exact solution named as `Barenblatt-Kompaneets-Zeldovich solution` has the form:\n",
    "$$\n",
    "u_{\\text{exact}}(x, t) = \\frac{1}{t^\\alpha} \\left( b - \\frac{m-1}{2m} \\beta \\frac{\\| x \\|^2}{t^{2\\beta}} \\right) _{+}^{\\frac{1}{m-1}}\n",
    "$$\n",
    "where $x \\in \\mathbb{R}^n$, $\\| \\cdot \\|^2$ is the $l^2$- norm, $(\\cdot)_+$ is the positive part, and\n",
    "$$\n",
    "\\alpha = \\frac{n}{n(m-1) +2}, ~~~~\\beta = \\frac{1}{n(m-1)+2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262c507-017b-4e30-b762-98e3d7172245",
   "metadata": {},
   "source": [
    "### Physics-Informed Neural Network Loss Term\n",
    "As the preliminary famework for building PINN, we use the following loss terms to optimize total loss by `.backward()` at the end of each training iteration. The 1-dimensional spatial domain $\\Omega := [-5, 5]$ and time domain is $[0,T]$ where $T$ is ending time.\n",
    "- Loss on domain\n",
    "  $$\n",
    "  L_{p} = u(x,t)_tt - u(x,t)^m, ~~~~ (x, t) \\in [-5, 5] \\times [0, T]\n",
    "  $$\n",
    "- Loss derived from initial condition\n",
    "  $$\n",
    "  L_i = u_{\\text{exact}}(x,.01),  ~~~~ x \\in [-5, 5]\n",
    "  $$\n",
    "- Loss derived from boundary condition - periodic boundary\n",
    "  $$\n",
    "  L_{b1} = u(-5,t) - u(5,t)\n",
    "  $$\n",
    "  $$\n",
    "  {L}_{b2} = u_x(-5,t) - u_x(5,t),  ~~~~ t \\in [0, T]\n",
    "  $$\n",
    "And, the basic total loss is set as follows.\n",
    "$$\n",
    "\\implies L_t = L_{p} + 1000 L_i + L_{b1} + L_{b2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b8228-94fd-4123-b9ec-64c774414e8e",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "This is the base line experiment using the basic total loss $L_t$. In this notebook we use 6 hidden layers with neuron size 100 for the `forward` pass of the neural network and `torch.tanh`activation function on each layer. In this experiment, 12000, 150 and 200 collocation points collocation points were used for computing $L_p$, $L_{b1}+{L}_{b2}$ and $L_i$ respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42044c65-9abf-4888-b241-a24a73510e6f",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "This is the classical experiment for observing the efficiency of adaptive resampling. we splited the basic total loss $L_t$ into the basic loss term and resampled loss term according to the upper 10% high-loss points:\n",
    "$$\n",
    "\\implies L_t = L_{p1} + L_{p2} + 1000 (L_{i1} + L_{i2}) + L_{b1} + L_{b2}\n",
    "$$\n",
    "where $L_{p1} = L_{p}$, $L_{i1} = L_{i}$. And, $L_{p2}$, $L_{i2}$ are pde loss term and initial loss term from the high-loss resampling for the upper 10% from each term. The collocation points numbers are 8000 for $L_{p1}$, 4000 for $L_{p2}$, 150 for $L_{i1}$, 50 for $L_{i2}$ and 150 for $L_{b1} + L_{b2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d95a1-5866-4733-9393-ce3779d12cd4",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "This is the new resampling method in MCMC process, **likelihood-free MCMC**, which is inspired by Metropolis Hasting and probability density approximation to the gradient of eneery $\\frac{\\partial E}{\\partial x}$. By employing this method, we give more training collocation points on the rapidley changing domain area powered by fast change of energy value. Similar to the Experiment2, to compare this with the previous experiment, we splited the basic total loss $L_t$ in experiment 1 into the basic loss term and loss term resampled from this likelihood-free MCMC:\n",
    "$$\n",
    "\\implies L_t = L_{p1} + 100L_{p3} + 1000 (L_{i1} + L_{i3}) + L_{b1} + L_{b2}\n",
    "$$\n",
    "where $L_{p1} = L_{p}$, $L_{i1} = L_{i}$. And, $L_{p3}$, $L_{i3}$ are pde loss term and initial loss term from the likelihood-free MCMC. The collocation points numbers are 8000 for $L_{p1}$, 4000 for $L_{p3}$, 150 for $L_{i1}$, 50 for $L_{i3}$ and 150 for $L_{b1} + L_{b2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cc8b5-f1fb-442a-b2a1-2d6f9ea9d78c",
   "metadata": {},
   "source": [
    "### Algorithm: Likelihodd-Free MCMC(LF Metropolis-Hastings) sampling\n",
    "\n",
    "We are given data $\\theta = {\\theta_1, \\theta_2, ..., \\theta_n}$ considered as likelihood of $x = {x_1, x_2, ..., x_n}$. We assume that the likelihood $\\theta$ is generalized as $x$ by using KDE likelihood on $\\theta$ and neighborhood points for each of $\\theta$. With this condition, we will accept or reject proposal $(\\theta', x')$ by using acceptance ratio $\\alpha$ where $\\theta' = {\\theta_1', \\theta_2', ..., \\theta_n'}$. Since $\\theta :=  From the current algorithm sate $(\\theta, x)$, a new parameter vetor $\\theta '$ is drawn from a proposal distribution $q(\\theta ' | \\theta)$.\n",
    "\n",
    "**Input**:  \n",
    "- Target distribution $\\pi(x, t) := |u(x, t) u_x(x, t)|$ (Normalized $\\frac{d E(u)}{dx} = m u^{m-1}u_x$)  \n",
    "- Proposal distribution $q(x' | x) ~ N(\\pi(x,t), 0.075)$  \n",
    "- Initial value $x_0 := \\pi (x,0)$  \n",
    "- Total number of approximating sampling iterations $N := 75$\n",
    "\n",
    "**Output**:  \n",
    "- Samples $\\hat{\\theta} = {\\hat{\\theta}_1, \\hat{\\theta}_2, ..., \\hat{\\theta}_n}$ approximating $\\pi(x, t)$\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Initialize k = 1 and observed data y  ← $\\pi(\\theta)$ from given $\\theta$  \n",
    "2. For step $k$ with given $(\\theta_{k},x_{k}) = (\\theta,x)$:\n",
    "    - (1) Extract generalized simulation data $x_i$ ←  $\\pi( B )$ for i = 1,2,...,n where B := {100 points in the closed disk centered $\\theta_i$ with radius 0.25}.  \n",
    "    - (2) Generate $\\theta'$ ∼ $q( \\theta' | \\theta)$\n",
    "    - (3) Similar to (1), generate generalized simulation data $x_i'$ ← $\\pi( B' )$ for i = 1,2,...,n where B' := {100 points in the closed disk centered $\\theta_i'$ with radius 0.25}.\n",
    "    - (4) Compute acceptance ratio:  \n",
    "      $$ \\alpha = \\min\\left(1, \\frac{\\pi_{\\epsilon}(y|x', \\theta') \\pi(\\theta')q(\\theta | \\theta')}{\\pi_{\\epsilon}(y|x, \\theta) \\pi(\\theta)q(\\theta' | \\theta)}\\right) $$\n",
    "      where $\\pi_{\\epsilon}(y|x, \\theta) = 1$ where $|\\text{KDE}(y, x)| < \\epsilon$, 0 otherwise for KDE() function computing KDE likelihood for a query y against group x of 100 neighborhood points of the query $\\theta$.\n",
    "    - (5) With probability $\\alpha$, accept $(\\theta_{k+1},x_{k+1}) = (\\theta', x')$ ; else, keep $(\\theta,x)$.\n",
    "3. Increment $k = k+1$ and go to 2. until $k = 75$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5e2ce-eaf4-46bb-ba53-5751a059bd9e",
   "metadata": {},
   "source": [
    "### Resampling Figures for each end time in time slice\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Loss_adaptive/2by10_adap.png\" width=\"750\"/><br/>\n",
    "      <small>Figure 2: Loss adaptive</small>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Energy_adaptive_std.075/2by10_adap.png\" width=\"750\"/><br/>\n",
    "      <small>Figure 3: Energy adaptive</small>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b6d62-7da5-4558-96fc-d6aa34ff940f",
   "metadata": {},
   "source": [
    "### Plot Results\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Loss_adaptive/3D_7_25_18h_end1.2.png\" width=\"750\"/><br/>\n",
    "      <small>Figure 1.1: Loss adaptive</small>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Energy_adaptive_std.075/3D_7_28_7h_end1.2.png\" width=\"750\"/><br/>\n",
    "      <small>Figure 1.2: Energy adaptive</small>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Loss_adaptive/2D_7_25_18h_end1.2_2d.png\" width=\"850\"/><br/>\n",
    "      <small>Figure 2.1: Loss adaptive</small>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"Energy_adaptive_std.075/2D_7_28_7h_end1.2_2d.png\" width=\"850\"/><br/>\n",
    "      <small>Figure 2.2: Energy adaptive</small>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee88ab1-5714-4cd6-b123-e4b224b5c7bd",
   "metadata": {},
   "source": [
    "### Error Value Results\n",
    "\n",
    "| Error Type        | Experiment 1 : Baseline | Experiment 2 : Loss Adpative | Experiment 3 : Energy Adpative |\n",
    "|-------------------|--------------------|--------------------|--------------------|\n",
    "| L∞ L∞ Error (u)      | value₁₁           | 0.46           | 0.44          |\n",
    "| L∞ Error (u)     | value₂₁           | 0.002141  (relative 0.020912)      | 0.001767  (relative 0.017264) |\n",
    "| L2 Error (u)      | value₃₁           |  0.002421 (relative 0.016163) | 0.002047 (relative 0.013662) |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b4c7d-7c29-43dd-b643-1cf88177cced",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Porous_medium_equation  \n",
    "[2] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng (eds.). *Handbook of Markov Chain Monte Carlo*. Chapman & Hall/CRC, 2011.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64603fd2",
   "metadata": {},
   "source": [
    "# Gradient Blow-up in PINN Simulations for rising-bubble Systems with Moving Boundaries\n",
    "\n",
    "***Focused on the torch.Tensor.backward()***\n",
    "    \n",
    "**Woojeong Kim** *8/7/2024*\n",
    "\n",
    "    \n",
    "#### Introduction\n",
    "The simulation of two-phase fluid systems using the Navier-Stokes-Cahn-Hilliard (NSCH) equations presents significant challenges, particularly when dealing with moving boundaries. One prominent example is the 2-dimensional rising-bubble system in two fluids. In this PDE system simulation, the density discontinuity at the moving boundary can cause **blow-up gradients problem** in the neural network forward pass, complicating the training process of Physics-Informed Neural Networks (PINNs). This writing explores the difficulties encountered in such simulations and proposes a method to overcome these challenges using `Normalize initial input variable`, `Normalization on a layer`, `Adjust domain range for the loss computation`,`gradient clipping` and `adaptive loss weighting`.\n",
    "\n",
    "#### Blow-up gradient phenominon in PINN Simulation of NSCH Systems\n",
    "The primary difficulty in simulating NSCH systems with moving boundaries arises from the density discontinuity at the interface of two fluids. On the 2-dimensional domain, density is our goal to expect as simulation result as a variable in neural network forward pass. The discontinuity of this variable leads to gradients that can blow up during the neural network forward pass since two different values in small region brings enormous increasing gradient theoretically. Specifically, the density variable, \\(\\rho\\), exhibits sharp changes at the moving boundary of bubble border line becuase the rising-bubble is located between two different fluids with different density for each. \n",
    "\n",
    "This rapid change of density causes the gradient of the density variable to become excessively large. When the loss function includes terms dependent on these gradients, the resulting gradients during backpropagation can become unmanageable, leading to errors in loss computation and training instability.\n",
    "\n",
    "#### Graph Recording for Gradient Computation\n",
    "During the backward pass, the gradients of the neural network parameters are computed using automatic differentiation. This process involves constructing a computational graph during the forward pass, which records the operations performed on the inputs. When the `.backward()` command is called, the chain rule is applied to this graph to compute the gradients of the loss with respect to each variable.\n",
    "\n",
    "#### Issues with Blow-Up Gradients\n",
    "When gradients blow up due to the density discontinuity, the loss term can become infinite or undefined, causing errors in the backward pass. This issue makes it impossible to compute meaningful gradients, halting the training process. Consequently, the PINN model fails to complete on learning an accurate solution to the NSCH equations.\n",
    "\n",
    "#### Gradient Clipping as a Solution\n",
    "To address the blow-up gradient problem, we implement gradient clipping between the loss computation and the backward operation. Gradient clipping involves setting a threshold beyond which gradients are scaled down to a manageable size. This technique ensures that no gradient exceeds a predefined maximum value, preventing the loss term from becoming infinite.\n",
    "\n",
    "```python\n",
    "# Example code for gradient clipping in PyTorch\n",
    "import torch\n",
    "\n",
    "# Assuming loss is computed\n",
    "loss = compute_loss()\n",
    "\n",
    "# Perform gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "#### Adjusting Coefficients for Fluid Interfaces\n",
    "By bounding the gradient of the density variable in the forward pass, we can adaptively adjust the coefficients for loss terms on different regions in the domain. Specifically, we assign different weights to the loss terms for the interior of each fluid and the moving boundary interface.\n",
    "\n",
    "- **Interior of Each Fluid**: In these regions, the collocation points should primarily focus on minimizing the Navier-Stokes PDE (NSpde) loss term. To mitigate the effect of large density gradients on the boundary of each fluid, we use an adaptive coefficient:\n",
    "  $$\n",
    "  \\text{coefficient}_{\\text{NSpde}} = \\frac{1}{1 + 0.1 \\cdot \\nabla \\rho}\n",
    "  $$\n",
    "  This coefficient reduces the influence of high gradients, stabilizing the loss term.\n",
    "\n",
    "- **Moving Boundary Interface**: At the interface of moving boundary of bubble, the primary concern is the surface tension force and the phase field's accuracy. We assign a different adaptive coefficient to these terms:\n",
    "  $$\n",
    "  \\text{coefficient}_{\\text{boundary}} = \\frac{\\text{difference of density in small region centered at each training point}}{0.00001 + \\nabla \\rho}\n",
    "  $$\n",
    "  This coefficient emphasizes the importance of accurately recognizing only the interface(Moving boundary of rising bubble) dynamics while preventing blow-up gradients. This helps us to differentiate the boundary training points and non-boundary training points through 2-dimensional domain for training collocation points since difference of density of the non-boundary training collocation points is zero and the one of the boundary training collocation points is non-zero. After this recognizing only the collocation points on boundary collocation points by giving nonzero coefficient as weight of loss term, the denominator \\( 0.00001 + \\nabla \\rho \\) is trainined to be small as a part of the process for decreasing this loss term while neural network training.\n",
    "\n",
    "#### Implementation Strategy\n",
    "To implement this strategy, we modify the loss function to include these adaptive coefficients. The loss function for the PINN model is thus a weighted sum of the NSpde loss term and the surface tension force loss term, with the weights dynamically adjusted based on the gradient of the density variable.\n",
    "\n",
    "```python\n",
    "# Example code for adaptive loss weighting\n",
    "def compute_adaptive_loss(u, v, rho, surface_tension_error):\n",
    "    grad_rho = torch.autograd.grad(rho.sum(), inputs, create_graph=True)[0]\n",
    "    coeff_NSpde = 1 / (1 + 0.1 * grad_rho)\n",
    "    coeff_boundary = (rho_diff_small_region) / (0.00001 + grad_rho)\n",
    "    \n",
    "    loss_NSpde = compute_NSpde_loss(u, v, rho) * coeff_NSpde\n",
    "    loss_boundary = compute_boundary_loss(surface_tension_error) * coeff_boundary\n",
    "    \n",
    "    total_loss = loss_NSpde + loss_boundary\n",
    "    return total_loss\n",
    "```\n",
    "\n",
    "#### Conclusion\n",
    "By employing gradient clipping and adaptive loss weighting, we can effectively manage the challenges posed by density discontinuities in NSCH systems with moving boundaries. This approach allows the PINN model to maintain stable training, accurately capture the dynamics of two-phase fluids, and reduce loss errors. These techniques, grounded in the principles of automatic differentiation and gradient management, offer a robust solution for simulating complex fluid systems with moving boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab47367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (mydev-env)",
   "language": "python",
   "name": "mydev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
